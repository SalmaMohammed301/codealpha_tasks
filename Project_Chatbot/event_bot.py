import nltk
nltk.download('punkt')
nltk.data.path.append('C:\\\\Users\\\\skandr store\\\\AppData\\\\Roaming\\\ltk_data\\\\tokenizers\\\\punkt')
nltk.download('stopwords')
import textdistance
import re
import time
import sys
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from gensim.models import Word2Vec
from difflib import SequenceMatcher
from nltk.stem.isri import ISRIStemmer
from nltk.tokenize import word_tokenize
from langdetect import detect
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
qa_pairs_ar = {
    "ÙƒÙŠÙ Ø£Ø­Ø¬Ø² Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ":
    "\u202BğŸ“… Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ 'Start Planning' ÙˆØ§Ù…Ù„Ø£ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ù„Ù„Ø­Ø¬Ø² \u202C.",
    "Ø£ÙŠÙ† Ø£Ø¬Ø¯ ÙƒÙ„ Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§ØªØŸ": 
    "\u202BğŸ“‹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Events Ù‡ØªÙ„Ø§Ù‚ÙŠ ÙƒÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø­Ø¶Ø±ØªÙƒ Ø³Ø¬Ù„ØªÙ‡Ø§\u202C.",
    "ÙƒÙŠÙ Ø£Ø¶ÙŠÙ Ø®Ø¯Ù…Ø§Øª Ø²ÙŠ Ù…ÙŠÙƒØ¨ Ø£Ùˆ ØªØµÙˆÙŠØ± Ø§Ùˆ Ø¨Ø¯Ù„Ø©ØŸ":
    "ğŸ’„ Ø¨Ø¹Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ù‡ØªÙ‚Ø¯Ø± ØªØ®ØªØ§Ø± Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.",
    "ÙƒÙŠÙ Ø£Ø´ØºÙ„ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†ØŸ": 
    "\u202BğŸ”§ Ø§ÙØªØ­ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ø¶ØºØ· Start Planning ÙˆØ§Ø¨Ø¯Ø£ Ø¹Ù„Ù‰ Ø·ÙˆÙ„\u202C.",
    "ÙƒÙŠÙ Ø£Ø¹Ø¯Ù„ Ø£Ùˆ Ø£Ù„ØºÙŠ Ø§Ù„Ø­Ø¬Ø² ØŸ": 
    "\u202B ğŸ“Œ Ù…Ù† My Events ØªÙ‚Ø¯Ø± ØªØ¹Ø¯Ù„ Ø£Ùˆ ØªÙ„ØºÙŠ Ø¨Ø³Ù‡ÙˆÙ„Ø© \u202C.",
    "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ø¨Ø¯Ø£":
    "\u202Bâœ¨ Ù…ÙÙŠØ´ Ù…Ø´ÙƒÙ„Ø©! Ø§Ø¶ØºØ· Start Planning ÙˆØ§Ù…Ø´ÙŠ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\u202C."}
qa_pairs_en = {
    "How do I book an event?": "ğŸ“… Press 'Start Planning' in website 'EventEase' and fill in the event details.",
    "Where can I find my events?": "ğŸ“‹ Click on 'Events' in website 'EventEase' to see all your booked events.",
    "How do I add services like makeup or photography?": "ğŸ’„ After selecting the event type, you can choose services like makeup or photography.",
    "How to use the app?": "ğŸ”§ Open the app and click 'Start Planning'  in website 'EventEase' to begin.",
    "How to cancel or edit a booking?": "ğŸ“Œ Go to 'My Events' in website 'EventEase' to cancel or edit your booking.",
    "I don't know how to start": "âœ¨ No problem! Just click 'Start Planning' in website 'EventEase' and follow the steps."}
dialect_map_ar = {
    "Ø§Ø²Ø§ÙŠ": "ÙƒÙŠÙ",
    "Ù„ÙŠÙ‡": "Ù„Ù…Ø§Ø°Ø§",
    "Ø§ÙŠÙ‡": "Ù…Ø§",
    "Ø¹Ø§ÙŠØ²": "Ø£Ø±ÙŠØ¯",
    "Ø¹Ø§ÙˆØ²Ù‡": "Ø£Ø±ÙŠØ¯",
    "Ø§Ø­Ø¬Ø²": "Ø­Ø¬Ø²",
    "Ø­Ø¬Ø²Øª": "Ø­Ø¬Ø²",
    "Ø§ÙŠÙÙŠÙ†ØªØ³": "Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³",
    "Ø§ÙŠÙÙŠÙ†Øª": "Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª",
    "Ø£Ù„ØºÙ‰": "Ø§Ù„ØºÙŠ",
    "ÙÙŠÙ†": "Ø£ÙŠÙ†",
    "Ø¥Ù„ØºØ§Ø¡": "Ø§Ù„ØºØ§Ø¡"}
dialect_map_en={
    "wya": "where are you",
    "btw": "by the way",
    "u": "you",
    "r": "are",
    "idk": "I don't know"}
keywords_groups_ar = {
    "Ø§Ø³ØªØ®Ø¯Ø§Ù…": ["Ø§Ø³ØªØ®Ø¯Ù…", "Ø§Ø´ØºÙ„","Ø§Ø¨Ø¯Ø£","Ø§Ù„ØªØ·Ø¨ÙŠÙ‚","Ù…Ø´ØºÙ„","Ù…Ø´ Ø¹Ø§Ø±Ù Ø§Ø´ØºÙ„","Ù…Ø´ Ø¹Ø§Ø±ÙØ©","Ù…Ø´ÙƒÙ„Ø©","Ù…Ø´ÙƒÙ„Ù‡", "Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†"],
    "Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©": ["Ø§Ø­Ø¬Ø²","Ø­Ø¬Ø²","Ù…Ù†Ø§Ø³Ø¨Ø©", "ÙØ±Ø­", "Ø­ÙÙ„Ø©", "ØªØ®Ø±Ø¬", "Ø¹ÙŠØ¯Ù…ÙŠÙ„Ø§Ø¯","ÙƒØªØ¨ ÙƒØªØ§Ø¨","Ø®Ø·ÙˆØ¨Ø©","Ø²ÙØ§Ù","Ø¹ÙŠØ¯ Ù…ÙŠÙ„Ø§Ø¯","Ù‚Ø§Ø¹Ø©","Ù…Ø¤ØªÙ…Ø±Ø§Øª", "Ø­Ø¬Ø² Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"],
    "Ø®Ø¯Ù…Ø§Øª": ["ÙØ³ØªØ§Ù†", "Ø¨ÙŠÙˆØªÙŠ Ø³Ù†ØªØ±", "Ø­Ù„Ø§Ù‚","Ø¨Ø¯Ù„Ø©","Ø¨Ø¯Ù„Ù‡", "Ù…ÙŠÙƒØ¨", "ÙÙˆØªÙˆØ¬Ø±Ø§ÙØ±","ÙƒÙˆØ§ÙÙŠØ±Ø©","Ø¶ÙŠÙ","Ø¶ÙŠÙˆÙ","ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ§","ÙƒÙˆØ§ÙÙŠØ±","Ø¯Ø¹ÙˆØ©","Ø¯Ø¹ÙˆØ§Øª","Ø®Ø¯Ù…Ø©","Ø®Ø¯Ù…Ø§Øª","Ù…ÙŠÙƒØ¨ Ø§Ø±ØªÙŠØ³Øª","Ø§Ù„Ø¨Ø¯Ù„Ø©","Ø¯ÙŠ Ø¬ÙŠ", "ØªØµÙˆÙŠØ±","Ø­Ø¬Ø² Ù…ÙŠÙƒØ¨", "Ø§Ø­Ø¬Ø² Ù…ÙŠÙƒØ¨"],
    "Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³": ["Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª","Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª","Ù‚Ø§Ø¦Ù…Ø©","Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª","Ø§Ù„Ù„ÙŠ ÙØ§ØªØª","Ø³Ø¬Ù„Øª","Ù…Ù†Ø§Ø³Ø¨Ø§ØªÙŠ","Ù…Ø¹Ø§Ø¯","Ù…ÙˆØ§Ø¹ÙŠØ¯", "events", "evt"],
    "Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ": ["Ø§Ù„ØºØ§Ø¡ Ø­Ø¬Ø²","Ø§Ù„ØºÙŠ Ø­Ø¬Ø²","Ø§ÙƒØ¯","Ø§Ø¹Ø¯Ù„","Ø¥Ù„ØºØ§Ø¡", "Ø£Ù„ØºÙŠ", "Ø§Ù„ØºØ§Ø¡", "Ø£Ù„ØºÙŠÙ‡", "Ø£Ø¹Ø¯Ù„", "Ø£Ø¹Ø¯Ù„Ù‡", "Ø£Ø¹Ø¯Ù„Ù‡Ø§", "Ø£Ù„ØºÙˆÙ‡Ø§", "Ø¥Ù„ØºØ§Ø¤Ù‡", "Ø¥Ù„ØºØ§Ø¡Ù‡Ø§", "Ø¥Ù„ØºÙŠÙ‡", "Ø§Ø­Ø°Ù","ØªØ¹Ø¯ÙŠÙ„", "Ø¹Ø¯Ù„Øª", "Ø£Ø¹Ø¯Ù„", "ØºÙŠØ±", "ØºÙŠÙ‘Ø±","Ø§Ø­Ø¯Ø«","Ø§ØºÙŠØ±","ØªØºÙŠÙŠØ±","Ø´ÙŠÙ„ Ø§Ù„Ø­Ø¬Ø²","Ø§Ø´Ø·Ø¨","ØªØ­Ø¯ÙŠØ«","ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ø²","Ø§Ù„ØºÙŠ", "ØªÙ… Ø§Ù„Ø­Ø¬Ø²", "ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø­Ø¬Ø²"]
}
keywords_groups_en = {
    "admin_booking": ["edit","confirm","confirm booking","modify","delete","change","update","cancel","done"],
    "book_event": ["book event","book","venue","birthday", "wedding","party","hall","Conference","Marriage", "graduation", "engagement","create event","reserve","booking","schedule","register"],
    "events": ["events","my events","registered","events list","list","past","date","registered","Occasions","Dates"],
    "services": ["makeup", "photography", "dress","add","Makeup Artist","services","Hairdresser","Barber","Invitation","Invitations","Guest","Guests","dj","decoration","service","catering", "suit", "beauty center", "photographer"],
    "usage": ["how to use","app usage","help","use","run","application","I don't know how to run","Problem","I don't know","start","app","guide","using the app", "how does it work", "open the app"]
}
responses_ar = {
    "Ø§Ø³ØªØ®Ø¯Ø§Ù…": "ğŸ¤– Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† ØªÙØ¹Ù„Ù‡ Ù„ÙƒÙŠ Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ",
    "Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©": 
    "\u202BğŸ“… Ø¯ÙŠ Ù…Ù‡Ù…Ø© Ø³Ù‡Ù„Ø©! Ø§Ø¶ØºØ· 'Start Planning' ÙˆØ§Ù…Ù„Ø§ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\u202C.",
    "Ø®Ø¯Ù…Ø§Øª": 
    "\u202BğŸ’„ Ø§Ø­Ø¬Ø² Ø®Ø¯Ù…Ø§ØªÙƒ Ø¨Ø¹Ø¯ Ù…Ù„Ø¡ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ 'Start Planning' ÙÙŠ ÙˆÙŠØ¨ Ø³Ø§ÙŠØª 'EventEase' \u202C.",
    "Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³":
    "\u202BğŸ“‹ ØªÙ‚Ø¯Ø± ØªØ´ÙˆÙ ÙƒÙ„ Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ 'Events'  ÙÙŠ ÙˆÙŠØ¨ Ø³Ø§ÙŠØª 'EventEase'\u202C.",
    "Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ":
    "\u202BğŸ“Œ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø­Ø¬ÙˆØ²Ø§Øª (ØªØ¹Ø¯ÙŠÙ„ / Ø¥Ù„ØºØ§Ø¡)ØŒ ØªÙˆØ¬Ù‡ Ù„Ù‚Ø³Ù… 'My Events'  ÙˆØ§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ \u202C."}
responses_en = {
    "usage": "ğŸ”§ Open the app and click 'Start Planning' to begin in website 'EventEase'.",
    "book_event": "ğŸ“… Easy! Just click 'Start Planning' in website 'EventEase' and fill in the event details.",
    "services": "ğŸ’„ After selecting your event, you can add services like makeup or photography.",
    "events": "ğŸ“‹ You can view all your events by clicking on 'Events' in website 'EventEase'.",
    "admin_booking": "ğŸ“Œ To edit or cancel a booking, go to 'My Events' in website 'EventEase' and choose the option you need."}
abbreviations_map = {
    "evt": "Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª",
    "evts": "Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª",
    "resv": "Ø­Ø¬Ø²",
    "bday": "Ø¹ÙŠØ¯Ù…ÙŠÙ„Ø§Ø¯",
    "grad": "ØªØ®Ø±Ø¬",
    "app": "Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†",
    "svc": "Ø®Ø¯Ù…Ø§Øª",
    "mkp": "Ù…ÙŠÙƒØ¨",
    "ph": "ÙÙˆØªÙˆØ¬Ø±Ø§ÙØ±"}
vectorizer_ar = TfidfVectorizer()
question_vectors_ar = vectorizer_ar.fit_transform(list(qa_pairs_ar.keys()))
vectorizer_en = TfidfVectorizer()
question_vectors_en = vectorizer_en.fit_transform(list(qa_pairs_en.keys()))
stopwords_ar = set(stopwords.words("arabic"))
stop_words_en = set(stopwords.words('english'))
all_known_words = set()
stemmer = ISRIStemmer()
for group in list(keywords_groups_ar.values()) + list(keywords_groups_en.values()):
    all_known_words.update(group)
all_known_words.update(dialect_map_ar.keys())
all_known_words.update(dialect_map_en.keys())
all_known_words.update(abbreviations_map.keys())
def detect_language(text):
    try:
        arabic_chars = re.findall(r'[\u0600-\u06FF]', text)
        arabic_ratio = len(arabic_chars) / max(len(text), 1)
        if arabic_ratio > 0.3:
            return 'ar'
        language = detect(text)
        return 'ar' if language == 'ar' else 'en'
    except:
        return 'en'
def normalize_dialect(text):
    language = detect_language(text)
    if language == "ar":
        for word, replacement in dialect_map_ar.items():
            text = text.replace(word, replacement)
    elif language == "en":
        for word, replacement in dialect_map_en.items():
            text = text.replace(word, replacement)
    return text
def normalize_abbreviations(text):
    for abbr, full in abbreviations_map.items():
        text = text.replace(abbr, full)
    return text
def correct_word(word):
    best_match = word
    highest_score = 0
    for known_word in all_known_words:
        score = textdistance.jaro_winkler(word, known_word)
        if score > highest_score and score > 0.85:
            highest_score = score
            best_match = known_word
    return best_match
def correct_input_text(text):
    words = text.split()
    corrected_words = [correct_word(word) for word in words]
    return " ".join(corrected_words)   
def clean_input_keep_stopwords(user_input):
    words = user_input.split()
    keywords_in_input = []
    for word in words:
        if word not in stopwords_ar:
            keywords_in_input.append(word)
        else:
            keywords_in_input.append(word)
    return " ".join(keywords_in_input)
def apply_stemming(text):
    words = word_tokenize(text)
    stemmed = [stemmer.stem(word) for word in words]
    return " ".join(stemmed)  
def preprocess_text(text):
    language=detect_language(text)
    text = normalize_dialect(text)
    text = normalize_abbreviations(text)
    text = correct_input_text(text)
    text = clean_input_keep_stopwords(text)
    if language == "ar":
        text = apply_stemming(text)
    return text
qa_pairs_ar = {preprocess_text(k): v for k, v in qa_pairs_ar.items()}
questions_ar= list(qa_pairs_ar.keys())
answers_ar = list(qa_pairs_ar.values())
qa_pairs_en = {preprocess_text(k): v for k, v in qa_pairs_en.items()}
questions_en = list(qa_pairs_en.keys())
answers_en = list(qa_pairs_en.values())
keywords_groups_ar = {
    cat: [preprocess_text(p) for p in phrases]
    for cat, phrases in keywords_groups_ar.items()}
keywords_groups_en = {
    cat: [preprocess_text(p) for p in phrases]
    for cat, phrases in keywords_groups_en.items()}
def smart_reply(user_input, language):
    user_input_processed = preprocess_text(user_input)  
    user_embedding = get_embedding(user_input)
    if language == 'ar':
        user_vector_ar = vectorizer_ar.transform([user_input_processed])
        similarity_ar = cosine_similarity(user_vector_ar, question_vectors_ar)
        max_sim_index_ar = similarity_ar.argmax()
        max_sim_score_ar = similarity_ar[0, max_sim_index_ar]
        if max_sim_score_ar > 0.3:
            return answers_ar[max_sim_index_ar]
    else:
        lowered = user_input.lower()
        if any(word in lowered for word in keywords_groups_en['usage']):
            return responses_en["usage"]
        if any(word in lowered for word in keywords_groups_en['book_event']):
            return responses_en["book_event"]
        if any(word in lowered for word in keywords_groups_en['admin_booking']):
            return responses_en["admin_booking"]
        if any(word in lowered for word in keywords_groups_en['services']):
            return responses_en["services"]
        if any(word in lowered for word in keywords_groups_en['events']):
            return responses_en["events"]
        qa_embeddings = [get_embedding(q) for q in questions_en]
        similarities = [get_similarity(user_embedding, emb) for emb in qa_embeddings]
        max_sim_idx = max(range(len(similarities)), key=lambda i: similarities[i])
        max_sim_score = similarities[max_sim_idx]
        if max_sim_score > 0.65:
            return answers_en[max_sim_idx]
    return None    
def split_long_phrase(text):
    try:
        return sent_tokenize(text)
    except:
        return [text]  
def get_embedding(text):
    return embedding_model.encode(text)
def get_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]
def is_sarcastic(text):
    arabic_patterns = [
        "Ù‡Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨ÙŠØ´ØªØºÙ„", "ÙŠØ¹Ù†ÙŠ Ø£Ø¶ØºØ· Ø§Ù„Ø²Ø±", "Ø¨Ø§Ù„Ù†ÙŠØ©", "Ø²Ø±Ø§Ø± Ø³Ø­Ø±ÙŠ",
        "ÙŠØ´ØªØºÙ„ Ù„ÙˆØ­Ø¯Ù‡", "ÙƒÙ„ Ø­Ø§Ø¬Ø© ØªØªØ­Ù„", "Ø¨Ù„Ù…Ø³Ø© ÙˆØ§Ø­Ø¯Ø©", "ÙŠØ§ Ø³Ù„Ø§Ù…",
        "Ø£ÙƒÙŠØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚", "Ø£ÙƒÙŠØ¯ Ø¯ÙŠ", "Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚","Ø£ÙƒÙŠØ¯ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†","Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†", "Ù‡Ùˆ ÙÙŠÙ† Ø§Ù„Ø²Ø±Ø§Ø± Ø§Ù„Ø³Ø­Ø±ÙŠ",
        "Ø¹Ø§ÙŠØ² Ù…Ø¹Ø¬Ø²Ø§Øª", "Ø¢Ù‡ Ø·Ø¨Ø¹Ø§", "Ø¹Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©", "ÙˆØ§Ùˆ",
        "ÙŠØ§ Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù‡ÙˆÙ„Ø©"]
    arabic_keywords = [
        "ÙŠØ§ Ø³Ù„Ø§Ù…", "Ø²Ø±Ø§Ø± Ø³Ø­Ø±ÙŠ", "ÙˆØ§Ùˆ","Ù„ÙˆØ­Ø¯Ù‡","Ø§Ù„Ø¬Ù†Ø©","Ø§Ù„Ù†Ø§Ø±","Ø§Ù„Ù‚Ù…Ø±","ÙŠØ§Ø³Ù„Ø§Ù…","Ø²Ø±","Ø²Ø±Ø§Ø±","Ø³Ø§Ø­Ø±","Ø§Ù„Ù†ÙŠØ©", "Ø¢Ù‡ Ø·Ø¨Ø¹Ø§", "Ø·Ø¨Ø¹Ø§"]
    english_patterns = [
        "oh sure", "of course it works", "magic button", "just like that", "by itself",
        "press a button and done", "god will do it", "it fixes everything", "wow so easy",
        "clearly the app", "wow amazing", "i just click and magic happens"]
    english_keywords = [
        "oh sure", "wow", "magic", "obviously","heaven","hell","magicain","button","moon", "right", "clearly"]
    text = text.lower()
    for pattern in arabic_patterns + english_patterns:
        if pattern in text:
            return True
    for word in arabic_keywords + english_keywords:
        if word in text:
            return True
    return False
def detect_sarcasm(user_input):
    if is_sarcastic(user_input):
        language = detect_language(user_input)
        if language == "en":
            return "ğŸ˜ Sounds a bit sarcastic! Want real help with something?"
        else:
            return "ğŸ˜ Ø´ÙƒÙ„Ùƒ Ø¨ØªØªÙƒÙ„Ù… Ø¨Ø³Ø®Ø±ÙŠØ©! Ù‡Ù„ ÙÙŠ Ø­Ø§Ø¬Ø© Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠÙ‡Ø§ ÙØ¹Ù„Ø§Ù‹ØŸ"
    return None 
def detect_questions(user_input, language='ar'):
    question_separators = ['ØŸ','ØŒ', '?', 'Ùˆ','Ø«Ù…','ÙƒÙ…Ø§Ù†','Ø¨Ø±Ø¶Ùˆ','ÙƒØ°Ù„Ùƒ','Ùˆ ÙƒÙ…Ø§Ù†','Ùˆ Ø¨Ø¹Ø¯ÙŠÙ†', 'and', 'also', 'as well', 'then']
    segments = re.split('|'.join(map(re.escape, question_separators)), user_input)
    segments = [s.strip() for s in segments if len(s.strip()) > 3]
    return segments 
def process_single_question(user_input):
    language = detect_language(user_input)
    user_input_norm = normalize_dialect(user_input)
    if language=="ar":
        user_input_norm = normalize_abbreviations(user_input_norm)
        user_input_norm = correct_input_text(user_input_norm)
        user_input_norm = clean_input_keep_stopwords(user_input_norm)
        user_input_norm = apply_stemming(user_input_norm)
    sarcasm_response = detect_sarcasm(user_input)
    if sarcasm_response:
        return sarcasm_response 
    thanks_keywords = ["Ø´ÙƒØ±Ø§", "Ø´ÙƒØ±Ù‹Ø§", "Ù…ØªØ´ÙƒØ±", "thx", "thanks", "thank you"]
    if any(kw in user_input.lower() for kw in thanks_keywords):
        return( "ğŸŒŸ Ø§Ù„Ø¹ÙÙˆ! Ø³Ø¹ÙŠØ¯ Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ğŸ˜Š" if language == "ar" else "ğŸŒŸ You're welcome! Happy to help ğŸ˜Š")     
    okay_keywords = [ "Ø§Ø´Ø·Ø§", "ØªÙ…Ø§Ù…", "okay"]
    if any(kw in user_input.lower() for kw in okay_keywords):
        return( "ğŸŒŸ  Ø³Ø¹ÙŠØ¯ Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ğŸ˜Š" if language == "ar" else "ğŸŒŸ Happy to help ğŸ˜Š")
    sentences=split_long_phrase(user_input_norm)
    user_embedding = get_embedding(user_input_norm)
    combined_keywords = {**keywords_groups_ar, **keywords_groups_en}
    responses_dict = responses_ar if language == "ar" else responses_en
    cancel_words = ["Ø§Ù„ØºØ§Ø¡", "Ø§Ù„ØºÙŠ","Ø¥Ù„ØºØ§Ø¡", "Ø£Ù„ØºÙŠ", "Ø£Ù„ØºÙŠÙ‡", "Ø£Ù„ØºÙˆÙ‡Ø§", "Ø¥Ù„ØºØ§Ø¤Ù‡", "Ø¥Ù„ØºØ§Ø¡Ù‡Ø§", "Ø¥Ù„ØºÙŠÙ‡", "Ø£Ù„ØºÙ‰", "Ø§Ø­Ø°Ù", "Ø­Ø°Ù", "remove", "cancel", "delete"]
    positive_words = ["ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø­Ø¬Ø²", "ØªÙ… Ø§Ù„Ø­Ø¬Ø²", "confirmed","confirm", "booked"]  
    is_cancel_request = any(word in user_input_norm for word in cancel_words) and not any(word in user_input_norm for word in positive_words)
    if is_cancel_request:
        cancel_targets = (
            keywords_groups_ar["Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©"] + keywords_groups_ar["Ø®Ø¯Ù…Ø§Øª"] + keywords_groups_ar["Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³"]
            if language == "ar"
            else keywords_groups_en["book_event"] + keywords_groups_en["services"] + keywords_groups_en["events"]
        )
        if any(t in user_input_norm for t in cancel_targets):
            return responses_ar["Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ"] if language == "ar" else responses_en["admin_booking"]
    editing_keywords =keywords_groups_en['admin_booking']+ keywords_groups_ar['Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ']
    editing = any(word in user_input_norm for word in editing_keywords)
    if editing :
        return responses_en["admin_booking"] if language == "en" else responses_ar["Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ"]    
    def is_generic(response):
        generic_keywords = ["Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ", "Ø£Ø³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ", "Ø§Ø³Ø£Ù„Ù†ÙŠ", "Ù…Ø³Ø§Ø¹Ø¯", "help", "assist", "anything else"]
        return any(kw in response.lower() for kw in generic_keywords)        
    all_responses=[]    
    seen_responses = set()
    for sentence in sentences:
        processed_sentence = preprocess_text(sentence)
        smart=smart_reply(processed_sentence,language)
        if smart and len(smart.split()) > 2 and not is_generic(smart):
            if smart not in seen_responses:
                all_responses.append(smart)
                seen_responses.add(smart)    
    if all_responses:
        return "\n---\n".join(all_responses)        
    tfidf_responses = []
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        if language == 'ar':
            sentence_vector_ar = vectorizer_ar.transform([sentence])
            similarity_ar = cosine_similarity(sentence_vector_ar, question_vectors_ar)
            max_sim_index_ar = similarity_ar.argmax()
            max_sim_score_ar = similarity_ar[0, max_sim_index_ar]
            if max_sim_score_ar > 0.4:
                tfidf_responses.append(list(qa_pairs_ar.values())[max_sim_index_ar])
        else:
            sentence_vector_en = vectorizer_en.transform([sentence])
            similarity_en = cosine_similarity(sentence_vector_en, question_vectors_en)
            max_sim_index_en = similarity_en.argmax()
            max_sim_score_en = similarity_en[0, max_sim_index_en] 
            if max_sim_score_en > 0.45 :
                tfidf_responses.append(list(qa_pairs_en.values())[max_sim_index_en])
    tfidf_responses = list(dict.fromkeys(tfidf_responses))  
    tfidf_filtered = []
    tfidf_embeddings = []
    for r in tfidf_responses:
        if is_generic(r.strip()):
            continue
        emb = get_embedding(r)
        if any(cosine_similarity([emb], [prev])[0][0] > 0.87 for prev in tfidf_embeddings):
            continue
        tfidf_filtered.append((r, emb))
        tfidf_embeddings.append(emb)
    if tfidf_filtered:
        tfidf_filtered.sort(key=lambda x: cosine_similarity([user_embedding], [x[1]])[0][0], reverse=True)
        top_responses = [r for r, _ in tfidf_filtered[:3]]
        for r in top_responses:
            if r not in seen_responses:
                all_responses.append(r)
                seen_responses.add(r)
        return "\n---\n".join(all_responses)
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        category_scores = {}
        for category, phrases in combined_keywords.items():
            matched_keywords = [phrase for phrase in phrases if phrase.lower() in sentence.lower()]
            if matched_keywords:
                priority_weight = 2 if category.lower() in ["book_event", "events", "Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©", "Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³"] else 1
                score = sum(2 if len(phrase.split()) > 1 else 1 for phrase in matched_keywords)
                total_score = score * priority_weight
                category_scores[category] = category_scores.get(category, 0) + total_score 
        if category_scores:
            sorted_category = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
            for category, score in sorted_category:
                if category in ["Ø®Ø¯Ù…Ø§Øª", "services"]:
                    stronger_categories = [cat for cat, s in sorted_category if s >= score and cat not in ["Ø®Ø¯Ù…Ø§Øª", "services"]]
                    if stronger_categories:
                        continue
                if category in responses_dict:
                    if responses_dict[category] not in seen_responses:
                        all_responses.append(responses_dict[category])
                        seen_responses.add(responses_dict[category])
                    break 
    if all_responses:
        return "\n---\n".join(all_responses)            
    return "ğŸ¤” Ù…Ø´ ÙØ§Ù‡Ù… Ø¹Ù„ÙŠÙƒ ÙƒÙˆÙŠØ³ØŒ Ù…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ø£ÙƒØªØ±ØŸ" if language == "ar" else "ğŸ¤” I didn't quite understand. Could you clarify?"
def get_response(user_input):
    language = detect_language(user_input)
    normalized_input = normalize_dialect(user_input)
    arabic_greetings = {"Ù…Ø±Ø­Ø¨Ø§","Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ø§Ù‡Ù„Ø§", "Ù‡Ù„Ø§","Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±","ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±","Ø¥Ø²ÙŠÙƒ","ØµØ¨Ø§Ø­ Ø§Ù„ÙÙ„ ÙŠØ§ Ø±ÙˆØ¨ÙˆØª","Ù…Ø³Ø§Ø¡ Ø§Ù„ÙÙ„ ÙŠØ§ Ø±ÙˆØ¨ÙˆØª", "Ù‡Ø§ÙŠ", "Ù‡ÙŠÙ„Ùˆ", "Ù‡Ø§Ù„Ùˆ"}
    english_greetings = {"hello", "hi", "hey", "hallo", "howdy","hey chatbot","good evening","good morning"}
    if language == "ar" and normalized_input.strip() in arabic_greetings:
        return "ğŸŒŸ Ù‡Ø§ÙŠ! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ù„Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ØªØ®Ø·Ø· Ù…Ù†Ø§Ø³Ø¨ØªÙƒ Ø¨Ø³Ù‡ÙˆÙ„Ø© ğŸ˜Š"
    elif language == "en" and normalized_input.strip().lower() in english_greetings:
        return "ğŸŒŸ Hey there! I'm here to help you plan your event with ease ğŸ˜Š"
    questions = detect_questions(user_input, language)

    if len(questions) == 1:
        return process_single_question(questions[0])
    else:
        final_responses = []
        seen = set()
        for q in questions:
            response = process_single_question(q)
            if response:
                for part in response.split("\n---\n"):
                    if part.strip() and part not in seen:
                        final_responses.append(part.strip())
                        seen.add(part.strip())
        if final_responses:
            return "\n---\n".join(final_responses)
        return "ğŸ¤” Ù…Ø´ ÙØ§Ù‡Ù… Ø¹Ù„ÙŠÙƒ ÙƒÙˆÙŠØ³ØŒ Ù…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ø£ÙƒØªØ±ØŸ" if language == "ar" else "ğŸ¤” I didn't quite understand. Could you clarify?"
def type_effect(text, delay=0):
    for char in text:
        sys.stdout.write(char)
        sys.stdout.flush()
        time.sleep(delay)
    print()  
def welcome_message():
    type_effect("ğŸ‰\u202B Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ ÙÙŠ Ø´Ø§Øª Ø¨ÙˆØª Event Ease!\u202C")
    type_effect("ğŸ‰ Welcome to the Event Ease Chatbot!")
    type_effect("ğŸ“± Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ù„Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ØªØ®Ø·Ø· Ù…Ù†Ø§Ø³Ø¨ØªÙƒ Ø¨Ø³Ù‡ÙˆÙ„Ø© â€“ Ø³ÙˆØ§Ø¡ ÙØ±Ø­ØŒ Ø®Ø·ÙˆØ¨Ø©ØŒ Ø¹ÙŠØ¯ Ù…ÙŠÙ„Ø§Ø¯ Ø£Ùˆ ØºÙŠØ±Ù‡Ù….")
    type_effect("ğŸ“± I'm here to help you easily plan your special events â€“ weddings, engagements, birthdays, and more.")
    type_effect("ğŸ’¡ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø­Ø¬Ø²ØŒ Ø§Ù„Ø®Ø¯Ù…Ø§ØªØŒ ØªØ¹Ø¯ÙŠÙ„ Ø£Ùˆ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§ØªØŒ Ø£Ùˆ Ø·Ø±ÙŠÙ‚Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.")
    type_effect("ğŸ’¡ Ask me about booking, services, editing or cancelling events, or how to use the app.")
    type_effect("ğŸŒ ØªÙ‚Ø¯Ø± ØªÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ.")
    type_effect("ğŸŒ You can talk to me in Arabic or English.")
    print("___________________________")
if __name__ == "__main__":
    welcome_message()
    while True:
        user_input = input("ğŸ“¨ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' / 'Ø§Ù†Ù‡Ø§Ø¡' / 'Ø¨Ø§ÙŠ' Ù„Ù„Ø®Ø±ÙˆØ¬)\nğŸ“¨ Type your question (or 'exit' / 'bye' to quit): ")
        arabic_goodbyes = ["Ø®Ø±ÙˆØ¬", "Ø§Ù†Ù‡Ø§Ø¡", "Ø¨Ø§ÙŠ","Ø³Ù„Ø§Ù…","Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©"]
        english_goodbyes = ["bye", "exit", "goodbye", "see you", "farewell"]
        normalized_input = user_input.strip().lower()
    
        if normalized_input in arabic_goodbyes:
            type_effect("ğŸ‘‹\u202B Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø´Ø§Øª Ø¨ÙˆØª Event Ease \u202C.") 
            break
        elif normalized_input in english_goodbyes:
            type_effect("ğŸ‘‹ See you again! Thanks for using the Event Ease Chatbot. Have a great day!")
            break
    
        else:
            response = get_response(user_input)
            type_effect("ğŸ¤–: " + response)
        
