{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0f1bc0-d21b-435b-92f5-06e6364a66b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\etc\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\etc\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\etc\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\etc\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\etc\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\etc\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\skandr\n",
      "[nltk_data]     store\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\skandr\n",
      "[nltk_data]     store\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in c:\\etc\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: gensim in c:\\etc\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\etc\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\etc\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\etc\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\etc\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\etc\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\etc\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\etc\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\etc\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\etc\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\etc\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\etc\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\etc\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\etc\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: nltk in c:\\etc\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: langdetect in c:\\etc\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: sentence-transformers in c:\\etc\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: textdistance in c:\\etc\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: click in c:\\etc\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\etc\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\etc\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\etc\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six in c:\\etc\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\etc\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\etc\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\etc\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\etc\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\etc\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\etc\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\etc\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\etc\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\etc\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\etc\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\etc\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\etc\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.data.path.append('C:\\\\Users\\\\skandr store\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt')\n",
    "nltk.download('stopwords')\n",
    "!pip install textdistance\n",
    "import textdistance\n",
    "!pip install gensim\n",
    "!pip install sentence-transformers\n",
    "!pip install nltk langdetect sentence-transformers textdistance\n",
    "import re\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "188b9672-a9df-47b9-b53d-c4ba90cc0874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰â€« Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ ÙÙŠ Ø´Ø§Øª Ø¨ÙˆØª Event Ease!â€¬\n",
      "ğŸ‰ Welcome to the Event Ease Chatbot!\n",
      "ğŸ“± Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ù„Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ØªØ®Ø·Ø· Ù…Ù†Ø§Ø³Ø¨ØªÙƒ Ø¨Ø³Ù‡ÙˆÙ„Ø© â€“ Ø³ÙˆØ§Ø¡ ÙØ±Ø­ØŒ Ø®Ø·ÙˆØ¨Ø©ØŒ Ø¹ÙŠØ¯ Ù…ÙŠÙ„Ø§Ø¯ Ø£Ùˆ ØºÙŠØ±Ù‡Ù….\n",
      "ğŸ“± I'm here to help you easily plan your special events â€“ weddings, engagements, birthdays, and more.\n",
      "ğŸ’¡ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø­Ø¬Ø²ØŒ Ø§Ù„Ø®Ø¯Ù…Ø§ØªØŒ ØªØ¹Ø¯ÙŠÙ„ Ø£Ùˆ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§ØªØŒ Ø£Ùˆ Ø·Ø±ÙŠÙ‚Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.\n",
      "ğŸ’¡ Ask me about booking, services, editing or cancelling events, or how to use the app.\n",
      "ğŸŒ ØªÙ‚Ø¯Ø± ØªÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ.\n",
      "ğŸŒ You can talk to me in Arabic or English.\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ“¨ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' / 'Ø§Ù†Ù‡Ø§Ø¡' / 'Ø¨Ø§ÙŠ' Ù„Ù„Ø®Ø±ÙˆØ¬)\n",
      "ğŸ“¨ Type your question (or 'exit' / 'bye' to quit):  Ø¹Ø§ÙˆØ² Ø§Ø­Ø¬Ø² Ø¨Ø¯Ù„Ø©\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤–: â€«ğŸ“… Ø¯ÙŠ Ù…Ù‡Ù…Ø© Ø³Ù‡Ù„Ø©! Ø§Ø¶ØºØ· 'Start Planning' ÙˆØ§Ù…Ù„Ø§ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©â€¬.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ“¨ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' / 'Ø§Ù†Ù‡Ø§Ø¡' / 'Ø¨Ø§ÙŠ' Ù„Ù„Ø®Ø±ÙˆØ¬)\n",
      "ğŸ“¨ Type your question (or 'exit' / 'bye' to quit):  Ø¹Ø§ÙˆØ²Ø© Ø§ØºÙŠØ± Ø§Ù„Ø­Ø¬Ø²\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤–: â€«ğŸ“Œ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø­Ø¬ÙˆØ²Ø§Øª (ØªØ¹Ø¯ÙŠÙ„ / Ø¥Ù„ØºØ§Ø¡)ØŒ ØªÙˆØ¬Ù‡ Ù„Ù‚Ø³Ù… 'My Events' ÙˆØ§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ â€¬.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ“¨ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' / 'Ø§Ù†Ù‡Ø§Ø¡' / 'Ø¨Ø§ÙŠ' Ù„Ù„Ø®Ø±ÙˆØ¬)\n",
      "ğŸ“¨ Type your question (or 'exit' / 'bye' to quit):  Ø¨Ø§ÙŠ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹â€« Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø´Ø§Øª Ø¨ÙˆØª Event Ease â€¬.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect\n",
    "\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "qa_pairs_ar = {\n",
    "    \"ÙƒÙŠÙ Ø£Ø­Ø¬Ø² Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ\":\n",
    "    \"\\u202BğŸ“… Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ 'Start Planning' ÙˆØ§Ù…Ù„Ø£ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ù„Ù„Ø­Ø¬Ø² \\u202C.\",\n",
    "    \"Ø£ÙŠÙ† Ø£Ø¬Ø¯ ÙƒÙ„ Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§ØªØŸ\": \n",
    "    \"\\u202BğŸ“‹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Events Ù‡ØªÙ„Ø§Ù‚ÙŠ ÙƒÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø­Ø¶Ø±ØªÙƒ Ø³Ø¬Ù„ØªÙ‡Ø§\\u202C.\",\n",
    "    \"ÙƒÙŠÙ Ø£Ø¶ÙŠÙ Ø®Ø¯Ù…Ø§Øª Ø²ÙŠ Ù…ÙŠÙƒØ¨ Ø£Ùˆ ØªØµÙˆÙŠØ± Ø§Ùˆ Ø¨Ø¯Ù„Ø©ØŸ\":\n",
    "    \"ğŸ’„ Ø¨Ø¹Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ù‡ØªÙ‚Ø¯Ø± ØªØ®ØªØ§Ø± Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.\",\n",
    "    \"ÙƒÙŠÙ Ø£Ø´ØºÙ„ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†ØŸ\": \n",
    "    \"\\u202BğŸ”§ Ø§ÙØªØ­ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ø¶ØºØ· Start Planning ÙˆØ§Ø¨Ø¯Ø£ Ø¹Ù„Ù‰ Ø·ÙˆÙ„\\u202C.\",\n",
    "    \"ÙƒÙŠÙ Ø£Ø¹Ø¯Ù„ Ø£Ùˆ Ø£Ù„ØºÙŠ Ø§Ù„Ø­Ø¬Ø² ØŸ\": \n",
    "    \"\\u202B ğŸ“Œ Ù…Ù† My Events ØªÙ‚Ø¯Ø± ØªØ¹Ø¯Ù„ Ø£Ùˆ ØªÙ„ØºÙŠ Ø¨Ø³Ù‡ÙˆÙ„Ø© \\u202C.\",\n",
    "    \"Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ø¨Ø¯Ø£\":\n",
    "    \"\\u202Bâœ¨ Ù…ÙÙŠØ´ Ù…Ø´ÙƒÙ„Ø©! Ø§Ø¶ØºØ· Start Planning ÙˆØ§Ù…Ø´ÙŠ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©\\u202C.\"}\n",
    "\n",
    "qa_pairs_en = {\n",
    "    \"How do I book an event?\": \"ğŸ“… Press 'Start Planning' and fill in the event details.\",\n",
    "    \"Where can I find my events?\": \"ğŸ“‹ Click on 'Events' to see all your booked events.\",\n",
    "    \"How do I add services like makeup or photography?\": \"ğŸ’„ After selecting the event type, you can choose services like makeup or photography.\",\n",
    "    \"How to use the app?\": \"ğŸ”§ Open the app and click 'Start Planning' to begin.\",\n",
    "    \"How to cancel or edit a booking?\": \"ğŸ“Œ Go to 'My Events' to cancel or edit your booking.\",\n",
    "    \"I don't know how to start\": \"âœ¨ No problem! Just click 'Start Planning' and follow the steps.\"}\n",
    "\n",
    "dialect_map_ar = {\n",
    "    \"Ø§Ø²Ø§ÙŠ\": \"ÙƒÙŠÙ\",\n",
    "    \"Ù„ÙŠÙ‡\": \"Ù„Ù…Ø§Ø°Ø§\",\n",
    "    \"Ø§ÙŠÙ‡\": \"Ù…Ø§\",\n",
    "    \"Ø¹Ø§ÙŠØ²\": \"Ø£Ø±ÙŠØ¯\",\n",
    "    \"Ø¹Ø§ÙˆØ²Ù‡\": \"Ø£Ø±ÙŠØ¯\",\n",
    "    \"Ø§Ø­Ø¬Ø²\": \"Ø­Ø¬Ø²\",\n",
    "    \"Ø­Ø¬Ø²Øª\": \"Ø­Ø¬Ø²\",\n",
    "    \"Ø§ÙŠÙÙŠÙ†ØªØ³\": \"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³\",\n",
    "    \"Ø§ÙŠÙÙŠÙ†Øª\": \"Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª\",\n",
    "    \"Ø£Ù„ØºÙ‰\": \"Ø§Ù„ØºÙŠ\",\n",
    "    \"ÙÙŠÙ†\": \"Ø£ÙŠÙ†\",\n",
    "    \"Ø¥Ù„ØºØ§Ø¡\": \"Ø§Ù„ØºØ§Ø¡\"}\n",
    "\n",
    "dialect_map_en={\n",
    "    \"wya\": \"where are you\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"idk\": \"I don't know\"}\n",
    "\n",
    "keywords_groups_ar = {\n",
    "    \"Ø§Ø³ØªØ®Ø¯Ø§Ù…\": [\"Ø§Ø³ØªØ®Ø¯Ù…\", \"Ø§Ø´ØºÙ„\",\"Ø§Ø¨Ø¯Ø£\",\"Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\",\"Ù…Ø´ØºÙ„\",\"Ù…Ø´ Ø¹Ø§Ø±Ù Ø§Ø´ØºÙ„\",\"Ù…Ø´ Ø¹Ø§Ø±ÙØ©\",\"Ù…Ø´ÙƒÙ„Ø©\",\"Ù…Ø´ÙƒÙ„Ù‡\", \"Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†\"],\n",
    "    \"Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©\": [\"Ø§Ø­Ø¬Ø²\",\"Ø­Ø¬Ø²\",\"Ù…Ù†Ø§Ø³Ø¨Ø©\", \"ÙØ±Ø­\", \"Ø­ÙÙ„Ø©\", \"ØªØ®Ø±Ø¬\", \"Ø¹ÙŠØ¯Ù…ÙŠÙ„Ø§Ø¯\",\"ÙƒØªØ¨ ÙƒØªØ§Ø¨\",\"Ø®Ø·ÙˆØ¨Ø©\",\"Ø²ÙØ§Ù\",\"Ø¹ÙŠØ¯ Ù…ÙŠÙ„Ø§Ø¯\",\"Ù‚Ø§Ø¹Ø©\",\"Ù…Ø¤ØªÙ…Ø±Ø§Øª\", \"Ø­Ø¬Ø² Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\"],\n",
    "    \"Ø®Ø¯Ù…Ø§Øª\": [\"ÙØ³ØªØ§Ù†\", \"Ø¨ÙŠÙˆØªÙŠ Ø³Ù†ØªØ±\", \"Ø­Ù„Ø§Ù‚\",\"Ø¨Ø¯Ù„Ø©\",\"Ø¨Ø¯Ù„Ù‡\", \"Ù…ÙŠÙƒØ¨\", \"ÙÙˆØªÙˆØ¬Ø±Ø§ÙØ±\",\"ÙƒÙˆØ§ÙÙŠØ±Ø©\",\"Ø¶ÙŠÙ\",\"Ø¶ÙŠÙˆÙ\",\"ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ§\",\"ÙƒÙˆØ§ÙÙŠØ±\",\"Ø¯Ø¹ÙˆØ©\",\"Ø¯Ø¹ÙˆØ§Øª\",\"Ø®Ø¯Ù…Ø©\",\"Ø®Ø¯Ù…Ø§Øª\",\"Ù…ÙŠÙƒØ¨ Ø§Ø±ØªÙŠØ³Øª\",\"Ø§Ù„Ø¨Ø¯Ù„Ø©\",\"Ø¯ÙŠ Ø¬ÙŠ\", \"ØªØµÙˆÙŠØ±\",\"Ø­Ø¬Ø² Ù…ÙŠÙƒØ¨\", \"Ø§Ø­Ø¬Ø² Ù…ÙŠÙƒØ¨\"],\n",
    "    \"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³\": [\"Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª\",\"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª\",\"Ù‚Ø§Ø¦Ù…Ø©\",\"Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª\",\"Ø§Ù„Ù„ÙŠ ÙØ§ØªØª\",\"Ø³Ø¬Ù„Øª\",\"Ù…Ù†Ø§Ø³Ø¨Ø§ØªÙŠ\",\"Ù…Ø¹Ø§Ø¯\",\"Ù…ÙˆØ§Ø¹ÙŠØ¯\", \"events\", \"evt\"],\n",
    "    \"Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ\": [\"Ø§Ù„ØºØ§Ø¡ Ø­Ø¬Ø²\",\"Ø§Ù„ØºÙŠ Ø­Ø¬Ø²\",\"Ø§ÙƒØ¯\",\"Ø§Ø¹Ø¯Ù„\",\"Ø¥Ù„ØºØ§Ø¡\", \"Ø£Ù„ØºÙŠ\", \"Ø§Ù„ØºØ§Ø¡\", \"Ø£Ù„ØºÙŠÙ‡\", \"Ø£Ø¹Ø¯Ù„\", \"Ø£Ø¹Ø¯Ù„Ù‡\", \"Ø£Ø¹Ø¯Ù„Ù‡Ø§\", \"Ø£Ù„ØºÙˆÙ‡Ø§\", \"Ø¥Ù„ØºØ§Ø¤Ù‡\", \"Ø¥Ù„ØºØ§Ø¡Ù‡Ø§\", \"Ø¥Ù„ØºÙŠÙ‡\", \"Ø§Ø­Ø°Ù\",\"ØªØ¹Ø¯ÙŠÙ„\", \"Ø¹Ø¯Ù„Øª\", \"Ø£Ø¹Ø¯Ù„\", \"ØºÙŠØ±\", \"ØºÙŠÙ‘Ø±\",\"Ø§Ø­Ø¯Ø«\",\"Ø§ØºÙŠØ±\",\"ØªØºÙŠÙŠØ±\",\"Ø´ÙŠÙ„ Ø§Ù„Ø­Ø¬Ø²\",\"Ø§Ø´Ø·Ø¨\",\"ØªØ­Ø¯ÙŠØ«\",\"ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ø²\",\"Ø§Ù„ØºÙŠ\", \"ØªÙ… Ø§Ù„Ø­Ø¬Ø²\", \"ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø­Ø¬Ø²\"]\n",
    "}\n",
    "\n",
    "keywords_groups_en = {\n",
    "    \"admin_booking\": [\"edit\",\"confirm\",\"confirm booking\",\"modify\",\"delete\",\"change\",\"update\",\"cancel\",\"done\"],\n",
    "    \"book_event\": [\"book event\",\"book\",\"venue\",\"birthday\", \"wedding\",\"party\",\"hall\",\"Conference\",\"Marriage\", \"graduation\", \"engagement\",\"create event\",\"reserve\",\"booking\",\"schedule\",\"register\"],\n",
    "    \"events\": [\"events\",\"my events\",\"registered\",\"events list\",\"list\",\"past\",\"date\",\"registered\",\"Occasions\",\"Dates\"],\n",
    "    \"services\": [\"makeup\", \"photography\", \"dress\",\"add\",\"Makeup Artist\",\"services\",\"Hairdresser\",\"Barber\",\"Invitation\",\"Invitations\",\"Guest\",\"Guests\",\"dj\",\"decoration\",\"service\",\"catering\", \"suit\", \"beauty center\", \"photographer\"],\n",
    "    \"usage\": [\"how to use\",\"app usage\",\"help\",\"use\",\"run\",\"application\",\"I don't know how to run\",\"Problem\",\"I don't know\",\"start\",\"app\",\"guide\",\"using the app\", \"how does it work\", \"open the app\"]\n",
    "}\n",
    "\n",
    "responses_ar = {\n",
    "    \"Ø§Ø³ØªØ®Ø¯Ø§Ù…\": \"ğŸ¤– Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† ØªÙØ¹Ù„Ù‡ Ù„ÙƒÙŠ Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ\",\n",
    "    \"Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©\": \n",
    "    \"\\u202BğŸ“… Ø¯ÙŠ Ù…Ù‡Ù…Ø© Ø³Ù‡Ù„Ø©! Ø§Ø¶ØºØ· 'Start Planning' ÙˆØ§Ù…Ù„Ø§ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\\u202C.\",\n",
    "    \"Ø®Ø¯Ù…Ø§Øª\": \n",
    "    \"\\u202BğŸ’„ Ø§Ø­Ø¬Ø² Ø®Ø¯Ù…Ø§ØªÙƒ Ø¨Ø¹Ø¯ Ù…Ù„Ø¡ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ 'Start Planning' \\u202C.\",\n",
    "    \"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³\":\n",
    "    \"\\u202BğŸ“‹ ØªÙ‚Ø¯Ø± ØªØ´ÙˆÙ ÙƒÙ„ Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ 'Events'\\u202C.\",\n",
    "    \"Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ\":\n",
    "    \"\\u202BğŸ“Œ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø­Ø¬ÙˆØ²Ø§Øª (ØªØ¹Ø¯ÙŠÙ„ / Ø¥Ù„ØºØ§Ø¡)ØŒ ØªÙˆØ¬Ù‡ Ù„Ù‚Ø³Ù… 'My Events' ÙˆØ§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ \\u202C.\"}\n",
    "\n",
    "responses_en = {\n",
    "    \"usage\": \"ğŸ”§ Open the app and click 'Start Planning' to begin.\",\n",
    "    \"book_event\": \"ğŸ“… Easy! Just click 'Start Planning' and fill in the event details.\",\n",
    "    \"services\": \"ğŸ’„ After selecting your event, you can add services like makeup or photography.\",\n",
    "    \"events\": \"ğŸ“‹ You can view all your events by clicking on 'Events'.\",\n",
    "    \"admin_booking\": \"ğŸ“Œ To edit or cancel a booking, go to 'My Events' and choose the option you need.\"}\n",
    "\n",
    "abbreviations_map = {\n",
    "    \"evt\": \"Ø§Ù„Ø§ÙŠÙÙŠÙ†Øª\",\n",
    "    \"evts\": \"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ§Øª\",\n",
    "    \"resv\": \"Ø­Ø¬Ø²\",\n",
    "    \"bday\": \"Ø¹ÙŠØ¯Ù…ÙŠÙ„Ø§Ø¯\",\n",
    "    \"grad\": \"ØªØ®Ø±Ø¬\",\n",
    "    \"app\": \"Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†\",\n",
    "    \"svc\": \"Ø®Ø¯Ù…Ø§Øª\",\n",
    "    \"mkp\": \"Ù…ÙŠÙƒØ¨\",\n",
    "    \"ph\": \"ÙÙˆØªÙˆØ¬Ø±Ø§ÙØ±\"}\n",
    "\n",
    "\n",
    "vectorizer_ar = TfidfVectorizer()\n",
    "question_vectors_ar = vectorizer_ar.fit_transform(list(qa_pairs_ar.keys()))\n",
    "vectorizer_en = TfidfVectorizer()\n",
    "question_vectors_en = vectorizer_en.fit_transform(list(qa_pairs_en.keys()))\n",
    "stopwords_ar = set(stopwords.words(\"arabic\"))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "all_known_words = set()\n",
    "stemmer = ISRIStemmer()\n",
    "for group in list(keywords_groups_ar.values()) + list(keywords_groups_en.values()):\n",
    "    all_known_words.update(group)\n",
    "all_known_words.update(dialect_map_ar.keys())\n",
    "all_known_words.update(dialect_map_en.keys())\n",
    "all_known_words.update(abbreviations_map.keys())\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        arabic_chars = re.findall(r'[\\u0600-\\u06FF]', text)\n",
    "        arabic_ratio = len(arabic_chars) / max(len(text), 1)\n",
    "        if arabic_ratio > 0.3:\n",
    "            return 'ar'\n",
    "        language = detect(text)\n",
    "        return 'ar' if language == 'ar' else 'en'\n",
    "    except:\n",
    "        return 'en'\n",
    "        \n",
    "def normalize_dialect(text):\n",
    "    language = detect_language(text)\n",
    "    if language == \"ar\":\n",
    "        for word, replacement in dialect_map_ar.items():\n",
    "            text = text.replace(word, replacement)\n",
    "    elif language == \"en\":\n",
    "        for word, replacement in dialect_map_en.items():\n",
    "            text = text.replace(word, replacement)\n",
    "    return text\n",
    "\n",
    "def normalize_abbreviations(text):\n",
    "    for abbr, full in abbreviations_map.items():\n",
    "        text = text.replace(abbr, full)\n",
    "    return text\n",
    "\n",
    "def correct_word(word):\n",
    "    best_match = word\n",
    "    highest_score = 0\n",
    "    for known_word in all_known_words:\n",
    "        score = textdistance.jaro_winkler(word, known_word)\n",
    "        if score > highest_score and score > 0.85:\n",
    "            highest_score = score\n",
    "            best_match = known_word\n",
    "    return best_match\n",
    "      \n",
    "def correct_input_text(text):\n",
    "    words = text.split()\n",
    "    corrected_words = [correct_word(word) for word in words]\n",
    "    return \" \".join(corrected_words)   \n",
    "    \n",
    "def clean_input_keep_stopwords(user_input):\n",
    "    words = user_input.split()\n",
    "    keywords_in_input = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_ar:\n",
    "            keywords_in_input.append(word)\n",
    "        else:\n",
    "            keywords_in_input.append(word)\n",
    "    return \" \".join(keywords_in_input)\n",
    "    \n",
    "def apply_stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed)\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    language=detect_language(text)\n",
    "    text = normalize_dialect(text)\n",
    "    text = normalize_abbreviations(text)\n",
    "    text = correct_input_text(text)\n",
    "    text = clean_input_keep_stopwords(text)\n",
    "    if language == \"ar\":\n",
    "        text = apply_stemming(text)\n",
    "    return text\n",
    "qa_pairs_ar = {preprocess_text(k): v for k, v in qa_pairs_ar.items()}\n",
    "questions_ar= list(qa_pairs_ar.keys())\n",
    "answers_ar = list(qa_pairs_ar.values())\n",
    "qa_pairs_en = {preprocess_text(k): v for k, v in qa_pairs_en.items()}\n",
    "questions_en = list(qa_pairs_en.keys())\n",
    "answers_en = list(qa_pairs_en.values())\n",
    "keywords_groups_ar = {\n",
    "    cat: [preprocess_text(p) for p in phrases]\n",
    "    for cat, phrases in keywords_groups_ar.items()}\n",
    "keywords_groups_en = {\n",
    "    cat: [preprocess_text(p) for p in phrases]\n",
    "    for cat, phrases in keywords_groups_en.items()}\n",
    "\n",
    "def smart_reply(user_input, language):\n",
    "    user_input_processed = preprocess_text(user_input)  \n",
    "    user_embedding = get_embedding(user_input)\n",
    "    if language == 'ar':\n",
    "        user_vector_ar = vectorizer_ar.transform([user_input_processed])\n",
    "        similarity_ar = cosine_similarity(user_vector_ar, question_vectors_ar)\n",
    "        max_sim_index_ar = similarity_ar.argmax()\n",
    "        max_sim_score_ar = similarity_ar[0, max_sim_index_ar]\n",
    "        if max_sim_score_ar > 0.3:\n",
    "            return answers_ar[max_sim_index_ar]\n",
    "    else:\n",
    "        lowered = user_input.lower()\n",
    "        if any(word in lowered for word in keywords_groups_en['usage']):\n",
    "            return responses_en[\"usage\"]\n",
    "        if any(word in lowered for word in keywords_groups_en['book_event']):\n",
    "            return responses_en[\"book_event\"]\n",
    "        if any(word in lowered for word in keywords_groups_en['admin_booking']):\n",
    "            return responses_en[\"admin_booking\"]\n",
    "        if any(word in lowered for word in keywords_groups_en['services']):\n",
    "            return responses_en[\"services\"]\n",
    "        if any(word in lowered for word in keywords_groups_en['events']):\n",
    "            return responses_en[\"events\"]\n",
    "        qa_embeddings = [get_embedding(q) for q in questions_en]\n",
    "        similarities = [get_similarity(user_embedding, emb) for emb in qa_embeddings]\n",
    "        max_sim_idx = max(range(len(similarities)), key=lambda i: similarities[i])\n",
    "        max_sim_score = similarities[max_sim_idx]\n",
    "        if max_sim_score > 0.65:\n",
    "            return answers_en[max_sim_idx]\n",
    "    return None\n",
    "        \n",
    "def split_long_phrase(text):\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except:\n",
    "        return [text]  \n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def get_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    \n",
    "def is_sarcastic(text):\n",
    "    arabic_patterns = [\n",
    "        \"Ù‡Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨ÙŠØ´ØªØºÙ„\", \"ÙŠØ¹Ù†ÙŠ Ø£Ø¶ØºØ· Ø§Ù„Ø²Ø±\", \"Ø¨Ø§Ù„Ù†ÙŠØ©\", \"Ø²Ø±Ø§Ø± Ø³Ø­Ø±ÙŠ\",\n",
    "        \"ÙŠØ´ØªØºÙ„ Ù„ÙˆØ­Ø¯Ù‡\", \"ÙƒÙ„ Ø­Ø§Ø¬Ø© ØªØªØ­Ù„\", \"Ø¨Ù„Ù…Ø³Ø© ÙˆØ§Ø­Ø¯Ø©\", \"ÙŠØ§ Ø³Ù„Ø§Ù…\",\n",
    "        \"Ø£ÙƒÙŠØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\", \"Ø£ÙƒÙŠØ¯ Ø¯ÙŠ\", \"Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\",\"Ø£ÙƒÙŠØ¯ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†\",\"Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ù„Ø§Ø¨Ù„ÙƒÙŠØ´Ù†\", \"Ù‡Ùˆ ÙÙŠÙ† Ø§Ù„Ø²Ø±Ø§Ø± Ø§Ù„Ø³Ø­Ø±ÙŠ\",\n",
    "        \"Ø¹Ø§ÙŠØ² Ù…Ø¹Ø¬Ø²Ø§Øª\", \"Ø¢Ù‡ Ø·Ø¨Ø¹Ø§\", \"Ø¹Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©\", \"ÙˆØ§Ùˆ\",\n",
    "        \"ÙŠØ§ Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù‡ÙˆÙ„Ø©\"]\n",
    "    arabic_keywords = [\n",
    "        \"ÙŠØ§ Ø³Ù„Ø§Ù…\", \"Ø²Ø±Ø§Ø± Ø³Ø­Ø±ÙŠ\", \"ÙˆØ§Ùˆ\",\"Ù„ÙˆØ­Ø¯Ù‡\",\"Ø§Ù„Ø¬Ù†Ø©\",\"Ø§Ù„Ù†Ø§Ø±\",\"Ø§Ù„Ù‚Ù…Ø±\",\"ÙŠØ§Ø³Ù„Ø§Ù…\",\"Ø²Ø±\",\"Ø²Ø±Ø§Ø±\",\"Ø³Ø§Ø­Ø±\",\"Ø§Ù„Ù†ÙŠØ©\", \"Ø¢Ù‡ Ø·Ø¨Ø¹Ø§\", \"Ø·Ø¨Ø¹Ø§\"]\n",
    "    english_patterns = [\n",
    "        \"oh sure\", \"of course it works\", \"magic button\", \"just like that\", \"by itself\",\n",
    "        \"press a button and done\", \"god will do it\", \"it fixes everything\", \"wow so easy\",\n",
    "        \"clearly the app\", \"wow amazing\", \"i just click and magic happens\"]\n",
    "    english_keywords = [\n",
    "        \"oh sure\", \"wow\", \"magic\", \"obviously\",\"heaven\",\"hell\",\"magicain\",\"button\",\"moon\", \"right\", \"clearly\"]\n",
    "    text = text.lower()\n",
    "    for pattern in arabic_patterns + english_patterns:\n",
    "        if pattern in text:\n",
    "            return True\n",
    "    for word in arabic_keywords + english_keywords:\n",
    "        if word in text:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def detect_sarcasm(user_input):\n",
    "    if is_sarcastic(user_input):\n",
    "        language = detect_language(user_input)\n",
    "        if language == \"en\":\n",
    "            return \"ğŸ˜ Sounds a bit sarcastic! Want real help with something?\"\n",
    "        else:\n",
    "            return \"ğŸ˜ Ø´ÙƒÙ„Ùƒ Ø¨ØªØªÙƒÙ„Ù… Ø¨Ø³Ø®Ø±ÙŠØ©! Ù‡Ù„ ÙÙŠ Ø­Ø§Ø¬Ø© Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠÙ‡Ø§ ÙØ¹Ù„Ø§Ù‹ØŸ\"\n",
    "    return None \n",
    "    \n",
    "def detect_questions(user_input, language='ar'):\n",
    "    question_separators = ['ØŸ','ØŒ', '?', 'Ùˆ','Ø«Ù…','ÙƒÙ…Ø§Ù†','Ø¨Ø±Ø¶Ùˆ','ÙƒØ°Ù„Ùƒ','Ùˆ ÙƒÙ…Ø§Ù†','Ùˆ Ø¨Ø¹Ø¯ÙŠÙ†', 'and', 'also', 'as well', 'then']\n",
    "    segments = re.split('|'.join(map(re.escape, question_separators)), user_input)\n",
    "    segments = [s.strip() for s in segments if len(s.strip()) > 3]\n",
    "    return segments \n",
    "    \n",
    "def process_single_question(user_input):\n",
    "    user_input_norm = normalize_dialect(user_input)\n",
    "    language = detect_language(user_input)\n",
    "    if language not in [\"ar\", \"en\"]:\n",
    "        return \"Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ Ø£ÙÙ‡Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù„ØºØ©. Ø£Ù†Ø§ Ø£ÙÙ‡Ù… ÙÙ‚Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.\\nSorry, I don't understand this language. I only understand Arabic and English.\"\n",
    "    if language==\"ar\":\n",
    "        user_input_norm = normalize_abbreviations(user_input_norm)\n",
    "        user_input_norm = correct_input_text(user_input_norm)\n",
    "        user_input_norm = clean_input_keep_stopwords(user_input_norm)\n",
    "        user_input_norm = apply_stemming(user_input_norm)\n",
    "        \n",
    "    sarcasm_response = detect_sarcasm(user_input)\n",
    "    if sarcasm_response:\n",
    "        return sarcasm_response    \n",
    "    thanks_keywords = [\"Ø´ÙƒØ±Ø§\", \"Ø´ÙƒØ±Ù‹Ø§\", \"Ù…ØªØ´ÙƒØ±\", \"thx\", \"thanks\", \"thank you\"]\n",
    "    if any(kw in user_input.lower() for kw in thanks_keywords):\n",
    "        return( \"ğŸŒŸ Ø§Ù„Ø¹ÙÙˆ! Ø³Ø¹ÙŠØ¯ Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ğŸ˜Š\" if language == \"ar\" else \"ğŸŒŸ You're welcome! Happy to help ğŸ˜Š\")\n",
    "    okay_keywords = [ \"Ø§Ø´Ø·Ø§\", \"ØªÙ…Ø§Ù…\", \"okay\"]\n",
    "    if any(kw in user_input.lower() for kw in okay_keywords):\n",
    "        return( \"ğŸŒŸ  Ø³Ø¹ÙŠØ¯ Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ğŸ˜Š\" if language == \"ar\" else \"ğŸŒŸ Happy to help ğŸ˜Š\")\n",
    "    sentences=split_long_phrase(user_input_norm)\n",
    "    user_embedding = get_embedding(user_input_norm)\n",
    "    combined_keywords = {**keywords_groups_ar, **keywords_groups_en}\n",
    "    responses_dict = responses_ar if language == \"ar\" else responses_en\n",
    "    cancel_words = [\"Ø§Ù„ØºØ§Ø¡\", \"Ø§Ù„ØºÙŠ\",\"Ø¥Ù„ØºØ§Ø¡\", \"Ø£Ù„ØºÙŠ\", \"Ø£Ù„ØºÙŠÙ‡\", \"Ø£Ù„ØºÙˆÙ‡Ø§\", \"Ø¥Ù„ØºØ§Ø¤Ù‡\", \"Ø¥Ù„ØºØ§Ø¡Ù‡Ø§\", \"Ø¥Ù„ØºÙŠÙ‡\", \"Ø£Ù„ØºÙ‰\", \"Ø§Ø­Ø°Ù\", \"Ø­Ø°Ù\", \"remove\", \"cancel\", \"delete\"]\n",
    "    positive_words = [\"ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø­Ø¬Ø²\", \"ØªÙ… Ø§Ù„Ø­Ø¬Ø²\", \"confirmed\",\"confirm\", \"booked\"]  \n",
    "    is_cancel_request = any(word in user_input_norm for word in cancel_words) and not any(word in user_input_norm for word in positive_words)\n",
    "    if is_cancel_request:\n",
    "        cancel_targets = (\n",
    "            keywords_groups_ar[\"Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©\"] + keywords_groups_ar[\"Ø®Ø¯Ù…Ø§Øª\"] + keywords_groups_ar[\"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³\"]\n",
    "            if language == \"ar\"\n",
    "            else keywords_groups_en[\"book_event\"] + keywords_groups_en[\"services\"] + keywords_groups_en[\"events\"]\n",
    "        )\n",
    "        if any(t in user_input_norm for t in cancel_targets):\n",
    "            return responses_ar[\"Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ\"] if language == \"ar\" else responses_en[\"admin_booking\"]\n",
    "    editing_keywords =keywords_groups_en['admin_booking']+ keywords_groups_ar['Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ']\n",
    "    editing = any(word in user_input_norm for word in editing_keywords)\n",
    "    if editing :\n",
    "        return responses_en[\"admin_booking\"] if language == \"en\" else responses_ar[\"Ø­Ø¬Ø²_Ø¥Ø¯Ø§Ø±ÙŠ\"]    \n",
    "    \n",
    "    def is_generic(response):\n",
    "        generic_keywords = [\"Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ\", \"Ø£Ø³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ\", \"Ø§Ø³Ø£Ù„Ù†ÙŠ\", \"Ù…Ø³Ø§Ø¹Ø¯\", \"help\", \"assist\", \"anything else\"]\n",
    "        return any(kw in response.lower() for kw in generic_keywords)        \n",
    "    all_responses=[]    \n",
    "    seen_responses = set()\n",
    "    for sentence in sentences:\n",
    "        processed_sentence = preprocess_text(sentence)\n",
    "        smart=smart_reply(processed_sentence,language)\n",
    "        if smart and len(smart.split()) > 2 and not is_generic(smart):\n",
    "            if smart not in seen_responses:\n",
    "                all_responses.append(smart)\n",
    "                seen_responses.add(smart)    \n",
    "    if all_responses:\n",
    "        return \"\\n---\\n\".join(all_responses)        \n",
    "    tfidf_responses = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        if language == 'ar':\n",
    "            sentence_vector_ar = vectorizer_ar.transform([sentence])\n",
    "            similarity_ar = cosine_similarity(sentence_vector_ar, question_vectors_ar)\n",
    "            max_sim_index_ar = similarity_ar.argmax()\n",
    "            max_sim_score_ar = similarity_ar[0, max_sim_index_ar]\n",
    "            if max_sim_score_ar > 0.4:\n",
    "                tfidf_responses.append(list(qa_pairs_ar.values())[max_sim_index_ar])\n",
    "        else:\n",
    "            sentence_vector_en = vectorizer_en.transform([sentence])\n",
    "            similarity_en = cosine_similarity(sentence_vector_en, question_vectors_en)\n",
    "            max_sim_index_en = similarity_en.argmax()\n",
    "            max_sim_score_en = similarity_en[0, max_sim_index_en] \n",
    "            if max_sim_score_en > 0.45 :\n",
    "                tfidf_responses.append(list(qa_pairs_en.values())[max_sim_index_en])\n",
    "    tfidf_responses = list(dict.fromkeys(tfidf_responses))  \n",
    "    tfidf_filtered = []\n",
    "    tfidf_embeddings = []\n",
    "    for r in tfidf_responses:\n",
    "        if is_generic(r.strip()):\n",
    "            continue\n",
    "        emb = get_embedding(r)\n",
    "        if any(cosine_similarity([emb], [prev])[0][0] > 0.87 for prev in tfidf_embeddings):\n",
    "            continue\n",
    "        tfidf_filtered.append((r, emb))\n",
    "        tfidf_embeddings.append(emb)\n",
    "    if tfidf_filtered:\n",
    "        tfidf_filtered.sort(key=lambda x: cosine_similarity([user_embedding], [x[1]])[0][0], reverse=True)\n",
    "        top_responses = [r for r, _ in tfidf_filtered[:3]]\n",
    "        for r in top_responses:\n",
    "            if r not in seen_responses:\n",
    "                all_responses.append(r)\n",
    "                seen_responses.add(r)\n",
    "        return \"\\n---\\n\".join(all_responses)\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        category_scores = {}\n",
    "        for category, phrases in combined_keywords.items():\n",
    "            matched_keywords = [phrase for phrase in phrases if phrase.lower() in sentence.lower()]\n",
    "            if matched_keywords:\n",
    "                priority_weight = 2 if category.lower() in [\"book_event\", \"events\", \"Ø­Ø¬Ø²_Ù…Ù†Ø§Ø³Ø¨Ø©\", \"Ø§Ù„Ø§ÙŠÙÙŠÙ†ØªØ³\"] else 1\n",
    "                score = sum(2 if len(phrase.split()) > 1 else 1 for phrase in matched_keywords)\n",
    "                total_score = score * priority_weight\n",
    "                category_scores[category] = category_scores.get(category, 0) + total_score \n",
    "        if category_scores:\n",
    "            sorted_category = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            for category, score in sorted_category:\n",
    "                if category in [\"Ø®Ø¯Ù…Ø§Øª\", \"services\"]:\n",
    "                    stronger_categories = [cat for cat, s in sorted_category if s >= score and cat not in [\"Ø®Ø¯Ù…Ø§Øª\", \"services\"]]\n",
    "                    if stronger_categories:\n",
    "                        continue\n",
    "                if category in responses_dict:\n",
    "                    if responses_dict[category] not in seen_responses:\n",
    "                        all_responses.append(responses_dict[category])\n",
    "                        seen_responses.add(responses_dict[category])\n",
    "                    break \n",
    "    if all_responses:\n",
    "        return \"\\n---\\n\".join(all_responses)            \n",
    "    return \"ğŸ¤” Ù…Ø´ ÙØ§Ù‡Ù… Ø¹Ù„ÙŠÙƒ ÙƒÙˆÙŠØ³ØŒ Ù…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ø£ÙƒØªØ±ØŸ\" if language == \"ar\" else \"ğŸ¤” I didn't quite understand. Could you clarify?\"\n",
    "    \n",
    "def get_response(user_input):\n",
    "    language = detect_language(user_input)\n",
    "    questions = detect_questions(user_input, language)\n",
    "\n",
    "    if len(questions) == 1:\n",
    "        return process_single_question(questions[0])\n",
    "    else:\n",
    "        final_responses = []\n",
    "        seen = set()\n",
    "    \n",
    "        for q in questions:\n",
    "            response = process_single_question(q)\n",
    "            if response:\n",
    "                for part in response.split(\"\\n---\\n\"):\n",
    "                    if part.strip() and part not in seen:\n",
    "                        final_responses.append(part.strip())\n",
    "                        seen.add(part.strip())\n",
    "    \n",
    "        if final_responses:\n",
    "            return \"\\n---\\n\".join(final_responses)\n",
    "        return \"ğŸ¤” Ù…Ø´ ÙØ§Ù‡Ù… Ø¹Ù„ÙŠÙƒ ÙƒÙˆÙŠØ³ØŒ Ù…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ø£ÙƒØªØ±ØŸ\" if language == \"ar\" else \"ğŸ¤” I didn't quite understand. Could you clarify?\"\n",
    "\n",
    "\n",
    "def type_effect(text, delay=0):\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(delay)\n",
    "    print()  \n",
    "\n",
    "def welcome_message():\n",
    "    type_effect(\"ğŸ‰\\u202B Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ ÙÙŠ Ø´Ø§Øª Ø¨ÙˆØª Event Ease!\\u202C\")\n",
    "    type_effect(\"ğŸ‰ Welcome to the Event Ease Chatbot!\")\n",
    "    type_effect(\"ğŸ“± Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ù„Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ØªØ®Ø·Ø· Ù…Ù†Ø§Ø³Ø¨ØªÙƒ Ø¨Ø³Ù‡ÙˆÙ„Ø© â€“ Ø³ÙˆØ§Ø¡ ÙØ±Ø­ØŒ Ø®Ø·ÙˆØ¨Ø©ØŒ Ø¹ÙŠØ¯ Ù…ÙŠÙ„Ø§Ø¯ Ø£Ùˆ ØºÙŠØ±Ù‡Ù….\")\n",
    "    type_effect(\"ğŸ“± I'm here to help you easily plan your special events â€“ weddings, engagements, birthdays, and more.\")\n",
    "    type_effect(\"ğŸ’¡ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø­Ø¬Ø²ØŒ Ø§Ù„Ø®Ø¯Ù…Ø§ØªØŒ ØªØ¹Ø¯ÙŠÙ„ Ø£Ùˆ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§ØªØŒ Ø£Ùˆ Ø·Ø±ÙŠÙ‚Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.\")\n",
    "    type_effect(\"ğŸ’¡ Ask me about booking, services, editing or cancelling events, or how to use the app.\")\n",
    "    type_effect(\"ğŸŒ ØªÙ‚Ø¯Ø± ØªÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ.\")\n",
    "    type_effect(\"ğŸŒ You can talk to me in Arabic or English.\")\n",
    "    print(\"___________________________\")\n",
    "welcome_message()\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ğŸ“¨ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' / 'Ø§Ù†Ù‡Ø§Ø¡' / 'Ø¨Ø§ÙŠ' Ù„Ù„Ø®Ø±ÙˆØ¬)\\nğŸ“¨ Type your question (or 'exit' / 'bye' to quit): \")\n",
    "    arabic_goodbyes = [\"Ø®Ø±ÙˆØ¬\", \"Ø§Ù†Ù‡Ø§Ø¡\", \"Ø¨Ø§ÙŠ\",\"Ø³Ù„Ø§Ù…\",\"Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©\"]\n",
    "    english_goodbyes = [\"bye\", \"exit\", \"goodbye\", \"see you\", \"farewell\"]\n",
    "    normalized_input = user_input.strip().lower()\n",
    "\n",
    "    if normalized_input in arabic_goodbyes:\n",
    "        type_effect(\"ğŸ‘‹\\u202B Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø´Ø§Øª Ø¨ÙˆØª Event Ease \\u202C.\") \n",
    "        break\n",
    "    elif normalized_input in english_goodbyes:\n",
    "        type_effect(\"ğŸ‘‹ See you again! Thanks for using the Event Ease Chatbot. Have a great day!\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        type_effect(\"ğŸ¤–: \" + response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431cea7-b694-4837-8f7d-4e7d73ecd06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
